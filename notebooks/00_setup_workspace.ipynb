{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e8f061-cda6-4592-aa1e-670cba7f100f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ⚠️ Before Running: Secret Scope Setup\n",
    "\n",
    "Before running this notebook, you need to set up a secret scope and secrets using the Databricks CLI:\n",
    "\n",
    "```bash\n",
    "# Create the secret scope\n",
    "databricks secrets create-scope --scope multai_scope\n",
    "\n",
    "# Add secrets to the scope (do this for each secret)\n",
    "databricks secrets put-secret multai_scope DATABRICKS_TOKEN\n",
    "databricks secrets put --scope multai_scope --key YFINANCE_API_KEY  # if using premium API\n",
    "```\n",
    "\n",
    "Required Secrets in `multai_scope`:\n",
    "- `DATABRICKS_TOKEN`: Your Databricks Personal Access Token\n",
    "- `YFINANCE_API_KEY`: Yahoo Finance API key (if using premium API)\n",
    "\n",
    "To verify the setup in this notebook, we'll check if we can access the secret scope (but not the secrets themselves for security)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b24de13-fd5a-4b09-bbb7-6599120a42f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify secret scope exists\n",
    "def check_secret_scope():\n",
    "    try:\n",
    "        # List all secret scopes\n",
    "        scopes = dbutils.secrets.listScopes()\n",
    "        \n",
    "        # Check if our scope exists\n",
    "        scope_exists = any(scope.name == 'multai_scope' for scope in scopes)\n",
    "        \n",
    "        if scope_exists:\n",
    "            print(\"✅ Secret scope 'multai_scope' is configured\")\n",
    "            # List keys in scope (this only shows key names, not values)\n",
    "            keys = dbutils.secrets.list('multai_scope')\n",
    "            print(\"\\nConfigured secrets:\")\n",
    "            for key in keys:\n",
    "                print(f\"- {key.key}\")\n",
    "        else:\n",
    "            print(\"❌ Secret scope 'multai_scope' not found!\")\n",
    "            print(\"Please run the CLI commands shown above to create the scope and add secrets.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking secret scope: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run the check\n",
    "check_secret_scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142dc6d7-d642-4130-9314-1bca67b784dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workspace Setup and Unity Catalog Configuration\n",
    "\n",
    "This notebook:\n",
    "1. Sets up Unity Catalog catalog and schemas\n",
    "2. Verifies workspace configuration\n",
    "3. Prints current user and workspace details\n",
    "\n",
    "**Note**: Requires appropriate Unity Catalog permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b5c197-03b2-4a4e-b1d6-21d3726fafea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Current User and Workspace Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185b332b-267a-4c92-a0b5-5eb27c9e19f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "# Get current user\n",
    "current_user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"Current User: {current_user}\")\n",
    "\n",
    "# Get workspace details\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "print(f\"Workspace URL: {workspace_url}\")\n",
    "\n",
    "# Print Spark version\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e5593d-4f71-4f8e-9b62-b61ab06a72e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Unity Catalog and Schemas\n",
    "\n",
    "Setting up the finance catalog with bronze and silver schemas for our medallion architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e3020c-11cd-4e4e-8a2b-18ecea34a9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to safely create catalog/schema\n",
    "def create_if_not_exists(entity_type: str, name: str, comment: str = None):\n",
    "    try:\n",
    "        if entity_type.lower() == 'catalog':\n",
    "            if comment:\n",
    "                spark.sql(f\"CREATE CATALOG IF NOT EXISTS {name} COMMENT '{comment}'\")\n",
    "            else:\n",
    "                spark.sql(f\"CREATE CATALOG IF NOT EXISTS {name}\")\n",
    "        elif entity_type.lower() == 'schema':\n",
    "            if comment:\n",
    "                spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {name} COMMENT '{comment}'\")\n",
    "            else:\n",
    "                spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {name}\")\n",
    "        print(f\"{entity_type.capitalize()} '{name}' created/verified successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating {entity_type} '{name}': {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56995059-f30f-44e9-a251-dda7704297df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create finance catalog\n",
    "\n",
    "create_if_not_exists(\n",
    "    'catalog',\n",
    "    'finance_catalog',\n",
    "    'Catalog for portfolio management data and analytics'\n",
    ")\n",
    "\n",
    "# Set the finance catalog as default\n",
    "spark.sql(\"USE CATALOG finance_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7606a07-55f9-46f1-a871-ea4f7fc8d016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create bronze schema\n",
    "create_if_not_exists(\n",
    "    'schema',\n",
    "    'finance_catalog.bronze',\n",
    "    'Raw financial data ingestion layer'\n",
    ")\n",
    "\n",
    "# Create silver schema\n",
    "create_if_not_exists(\n",
    "    'schema',\n",
    "    'finance_catalog.silver',\n",
    "    'Cleaned and transformed financial data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791c632a-d118-46fb-9e10-27dea835cedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify Setup\n",
    "\n",
    "List the created catalogs and schemas to verify the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb33ff9-695e-471b-9abd-f6dd5930aed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List available catalogs\n",
    "print(\"Available Catalogs:\")\n",
    "display(spark.sql(\"SHOW CATALOGS\"))\n",
    "\n",
    "# List schemas in finance_catalog\n",
    "print(\"\\nSchemas in finance_catalog:\")\n",
    "display(spark.sql(\"SHOW SCHEMAS IN finance_catalog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ba9110-4344-4b27-8395-fbd8651ec4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def verify_existence(\n",
    "    catalog_name: str,\n",
    "    schema_names: list = None\n",
    "):\n",
    "    # Check catalog\n",
    "    catalogs_df = spark.sql(\"SHOW CATALOGS\").filter(\n",
    "        f\"catalog = '{catalog_name}'\"\n",
    "    )\n",
    "    if catalogs_df.count() > 0:\n",
    "        print(f\"✅ Catalog '{catalog_name}' exists\")\n",
    "    else:\n",
    "        print(f\"❌ Catalog '{catalog_name}' not found!\")\n",
    "        return\n",
    "\n",
    "    # Check schemas if provided\n",
    "    if schema_names:\n",
    "        print(f\"\\nVerifying schemas in {catalog_name}:\")\n",
    "        schemas_df = spark.sql(\n",
    "            f\"SHOW SCHEMAS IN {catalog_name}\"\n",
    "        )\n",
    "        for schema in schema_names:\n",
    "            # Use only the schema name, not catalog.schema\n",
    "            schema_only = schema.split('.')[-1]\n",
    "            if schemas_df.filter(\n",
    "                f\"databaseName = '{schema_only}'\"\n",
    "            ).count() > 0:\n",
    "                print(f\"✅ Schema '{schema}' exists\")\n",
    "            else:\n",
    "                print(f\"❌ Schema '{schema}' not found!\")\n",
    "\n",
    "# Verify our specific catalog and schemas\n",
    "verify_existence(\n",
    "    'finance_catalog',\n",
    "    ['bronze', 'silver']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_setup_workspace",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
