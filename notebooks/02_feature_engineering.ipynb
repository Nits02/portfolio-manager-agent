{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58c3fae",
   "metadata": {},
   "source": [
    "# Feature Engineering for Portfolio Management\n",
    "\n",
    "This notebook demonstrates the FeatureEngineeringAgent functionality:\n",
    "1. Import and initialize the FeatureEngineeringAgent\n",
    "2. Process sample tickers (AAPL, MSFT) to create financial features\n",
    "3. Validate output and display sample data from Unity Catalog tables\n",
    "4. Analyze the generated features for ML readiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16418666",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and initialize Spark session if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5227482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom FeatureEngineeringAgent\n",
    "from agents.feature_engineering_agent import FeatureEngineeringAgent, FeatureEngineeringError\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Python path includes: {[p for p in sys.path if 'src' in p]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e71679",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session and FeatureEngineeringAgent\n",
    "\n",
    "Create Spark session and initialize the feature engineering agent with Unity Catalog configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef017e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session (if not already available in Databricks)\n",
    "# In Databricks, spark session is usually pre-configured\n",
    "try:\n",
    "    # Check if spark session already exists (common in Databricks)\n",
    "    spark_session = spark\n",
    "    print(\"‚úÖ Using existing Spark session from Databricks\")\n",
    "except NameError:\n",
    "    # Create new Spark session if not in Databricks environment\n",
    "    spark_session = SparkSession.builder \\\n",
    "        .appName(\"FeatureEngineering\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"‚úÖ Created new Spark session\")\n",
    "\n",
    "print(f\"Spark version: {spark_session.version}\")\n",
    "print(f\"Spark application: {spark_session.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FeatureEngineeringAgent\n",
    "print(\"üöÄ Initializing FeatureEngineeringAgent...\")\n",
    "\n",
    "# Create agent with Unity Catalog configuration\n",
    "feature_agent = FeatureEngineeringAgent(catalog=\"main\", schema=\"finance\")\n",
    "\n",
    "print(f\"‚úÖ FeatureEngineeringAgent initialized:\")\n",
    "print(f\"   - Catalog: {feature_agent.catalog}\")\n",
    "print(f\"   - Schema: {feature_agent.schema}\")\n",
    "print(f\"   - Target namespace: {feature_agent.catalog}.{feature_agent.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1833380",
   "metadata": {},
   "source": [
    "## 3. Check Available Raw Data\n",
    "\n",
    "Before running feature engineering, let's verify that the raw data tables exist in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56804b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available tables in the finance schema\n",
    "print(\"üìä Checking available tables in main.finance schema...\")\n",
    "\n",
    "try:\n",
    "    tables = spark_session.sql(\"SHOW TABLES IN main.finance\").collect()\n",
    "    \n",
    "    if tables:\n",
    "        print(\"\\nüìã Available tables:\")\n",
    "        for table in tables:\n",
    "            table_name = table['tableName']\n",
    "            print(f\"   - {table_name}\")\n",
    "            \n",
    "            # Check if it's a raw data table for our target tickers\n",
    "            if any(ticker.lower() in table_name.lower() for ticker in ['aapl', 'msft']):\n",
    "                # Show sample data\n",
    "                print(f\"     Sample data for {table_name}:\")\n",
    "                sample_df = spark_session.table(f\"main.finance.{table_name}\")\n",
    "                sample_df.select(\"ticker\", \"date\", \"close\", \"volume\").limit(3).show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No tables found in main.finance schema\")\n",
    "        print(\"   Make sure to run the data ingestion notebook first\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking tables: {str(e)}\")\n",
    "    print(\"   This might indicate Unity Catalog is not properly configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acce07",
   "metadata": {},
   "source": [
    "## 4. Run Feature Engineering\n",
    "\n",
    "Process AAPL and MSFT tickers to create financial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target tickers for feature engineering\n",
    "target_tickers = [\"AAPL\", \"MSFT\"]\n",
    "\n",
    "print(f\"üîß Starting feature engineering for: {', '.join(target_tickers)}\")\n",
    "print(f\"   Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    # Process tickers through feature engineering pipeline\n",
    "    results = feature_agent.process_tickers(target_tickers)\n",
    "    \n",
    "    print(\"\\n‚úÖ Feature engineering completed!\")\n",
    "    print(f\"   Duration: {(results['end_time'] - results['start_time']).total_seconds():.2f} seconds\")\n",
    "    print(f\"   Total features created: {results['total_features_created']}\")\n",
    "    \n",
    "    if results['processed_tickers']:\n",
    "        print(f\"   ‚úÖ Successfully processed: {', '.join(results['processed_tickers'])}\")\n",
    "    \n",
    "    if results['failed_tickers']:\n",
    "        print(f\"   ‚ùå Failed tickers: {', '.join(results['failed_tickers'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Feature engineering failed: {str(e)}\")\n",
    "    print(\"   Check the logs for detailed error information\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91495c01",
   "metadata": {},
   "source": [
    "## 5. Validate Output Tables\n",
    "\n",
    "Check that the feature tables were created successfully in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for newly created feature tables\n",
    "print(\"üîç Validating output tables...\")\n",
    "\n",
    "feature_tables = []\n",
    "for ticker in target_tickers:\n",
    "    table_name = f\"main.finance.features_{ticker}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists\n",
    "        table_exists = spark_session.catalog.tableExists(table_name)\n",
    "        \n",
    "        if table_exists:\n",
    "            print(f\"‚úÖ {table_name} exists\")\n",
    "            feature_tables.append(table_name)\n",
    "            \n",
    "            # Get table info\n",
    "            df = spark_session.table(table_name)\n",
    "            row_count = df.count()\n",
    "            col_count = len(df.columns)\n",
    "            \n",
    "            print(f\"   üìä Table stats: {row_count:,} rows, {col_count} columns\")\n",
    "            \n",
    "            # Show column names\n",
    "            print(f\"   üìã Columns: {', '.join(df.columns)}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå {table_name} does not exist\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking {table_name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüìà Total feature tables created: {len(feature_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a833d4",
   "metadata": {},
   "source": [
    "## 6. Display Sample Feature Data\n",
    "\n",
    "Show sample data from the generated feature tables to verify quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data from feature tables\n",
    "print(\"üìã Sample Feature Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for table_name in feature_tables:\n",
    "    ticker = table_name.split('_')[-1]  # Extract ticker from table name\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è {ticker} Features ({table_name})\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        df = spark_session.table(table_name)\n",
    "        \n",
    "        # Show recent data (last 5 rows)\n",
    "        print(\"\\nüìÖ Most Recent 5 Records:\")\n",
    "        df.orderBy(F.desc(\"date\")).limit(5).show(truncate=False)\n",
    "        \n",
    "        # Show feature summary statistics\n",
    "        print(\"\\nüìä Feature Statistics:\")\n",
    "        feature_cols = ['daily_return', 'moving_avg_7', 'moving_avg_30', 'volatility_7', 'momentum']\n",
    "        df.select(feature_cols).summary().show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error displaying data for {table_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145115b",
   "metadata": {},
   "source": [
    "## 7. Feature Quality Analysis\n",
    "\n",
    "Analyze the quality and completeness of generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a41131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature quality\n",
    "print(\"üî¨ Feature Quality Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for table_name in feature_tables:\n",
    "    ticker = table_name.split('_')[-1]\n",
    "    \n",
    "    print(f\"\\nüìà Analysis for {ticker}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        df = spark_session.table(table_name)\n",
    "        total_rows = df.count()\n",
    "        \n",
    "        # Check for null values in key features\n",
    "        feature_cols = ['daily_return', 'moving_avg_7', 'moving_avg_30', 'volatility_7', 'momentum']\n",
    "        \n",
    "        print(f\"üìä Data Completeness (out of {total_rows:,} total rows):\")\n",
    "        for col in feature_cols:\n",
    "            null_count = df.filter(F.col(col).isNull()).count()\n",
    "            non_null_count = total_rows - null_count\n",
    "            completeness = (non_null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "            \n",
    "            status = \"‚úÖ\" if completeness >= 95 else \"‚ö†Ô∏è\" if completeness >= 80 else \"‚ùå\"\n",
    "            print(f\"   {status} {col}: {completeness:.1f}% complete ({non_null_count:,} values)\")\n",
    "        \n",
    "        # Check date range\n",
    "        date_stats = df.select(\n",
    "            F.min(\"date\").alias(\"min_date\"),\n",
    "            F.max(\"date\").alias(\"max_date\"),\n",
    "            F.count(\"date\").alias(\"total_days\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\nüìÖ Date Range:\")\n",
    "        print(f\"   From: {date_stats['min_date']}\")\n",
    "        print(f\"   To: {date_stats['max_date']}\")\n",
    "        print(f\"   Total trading days: {date_stats['total_days']:,}\")\n",
    "        \n",
    "        # Feature value ranges\n",
    "        print(f\"\\nüìè Feature Ranges:\")\n",
    "        for col in ['daily_return', 'volatility_7', 'momentum']:\n",
    "            stats = df.select(\n",
    "                F.min(col).alias('min_val'),\n",
    "                F.max(col).alias('max_val'),\n",
    "                F.avg(col).alias('avg_val')\n",
    "            ).collect()[0]\n",
    "            \n",
    "            print(f\"   {col}: [{stats['min_val']:.4f}, {stats['max_val']:.4f}] (avg: {stats['avg_val']:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing {table_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9b149",
   "metadata": {},
   "source": [
    "## 8. Verification Queries\n",
    "\n",
    "Run some verification queries to ensure data consistency and feature correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e392488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run verification queries\n",
    "print(\"üîç Data Verification Queries\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if feature_tables:\n",
    "    # Query 1: Check if daily returns are calculated correctly\n",
    "    print(\"\\nüìä Query 1: Daily Return Calculation Verification\")\n",
    "    print(\"Checking if daily_return = (close - prev_close) / prev_close\")\n",
    "    \n",
    "    for table_name in feature_tables[:1]:  # Check first table only\n",
    "        ticker = table_name.split('_')[-1]\n",
    "        \n",
    "        verification_query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            date,\n",
    "            close,\n",
    "            LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date) as prev_close,\n",
    "            daily_return,\n",
    "            ROUND(\n",
    "                (close - LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date)) / \n",
    "                LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date), \n",
    "                6\n",
    "            ) as calculated_return\n",
    "        FROM {table_name}\n",
    "        WHERE date >= (SELECT MAX(date) - INTERVAL 7 DAYS FROM {table_name})\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = spark_session.sql(verification_query)\n",
    "            print(f\"\\n{ticker} - Recent daily returns:\")\n",
    "            result.show(truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in verification query: {str(e)}\")\n",
    "\n",
    "    # Query 2: Check moving averages\n",
    "    print(\"\\nüìà Query 2: Moving Average Verification\")\n",
    "    print(\"Checking 7-day and 30-day moving averages\")\n",
    "    \n",
    "    for table_name in feature_tables[:1]:  # Check first table only\n",
    "        ticker = table_name.split('_')[-1]\n",
    "        \n",
    "        ma_query = f\"\"\"\n",
    "        SELECT \n",
    "            date,\n",
    "            close,\n",
    "            moving_avg_7,\n",
    "            moving_avg_30,\n",
    "            ROUND(\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY ticker \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "                ), 2\n",
    "            ) as calculated_ma7\n",
    "        FROM {table_name}\n",
    "        WHERE date >= (SELECT MAX(date) - INTERVAL 10 DAYS FROM {table_name})\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = spark_session.sql(ma_query)\n",
    "            print(f\"\\n{ticker} - Recent moving averages:\")\n",
    "            result.show(truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in moving average query: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No feature tables available for verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c676a",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Summarize the feature engineering results and provide guidance for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üìã Feature Engineering Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(f\"\\n‚úÖ Feature Engineering Completed Successfully\")\n",
    "    print(f\"   - Target tickers: {', '.join(target_tickers)}\")\n",
    "    print(f\"   - Successfully processed: {len(results['processed_tickers'])} tickers\")\n",
    "    print(f\"   - Failed: {len(results['failed_tickers'])} tickers\")\n",
    "    print(f\"   - Total features created: {results['total_features_created']:,}\")\n",
    "    print(f\"   - Processing time: {(results['end_time'] - results['start_time']).total_seconds():.2f} seconds\")\n",
    "    \n",
    "    if results['processed_tickers']:\n",
    "        print(f\"\\nüìä Available Feature Tables:\")\n",
    "        for ticker in results['processed_tickers']:\n",
    "            print(f\"   - main.finance.features_{ticker}\")\n",
    "            \n",
    "    print(f\"\\nüéØ Created Features:\")\n",
    "    feature_list = [\n",
    "        \"daily_return (Daily price return)\",\n",
    "        \"moving_avg_7 (7-day moving average)\",\n",
    "        \"moving_avg_30 (30-day moving average)\",\n",
    "        \"volatility_7 (7-day rolling volatility)\",\n",
    "        \"momentum (Price momentum indicator)\",\n",
    "        \"feature_timestamp (Feature creation date)\"\n",
    "    ]\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        print(f\"   ‚úÖ {feature}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Feature engineering did not complete successfully\")\n",
    "    print(\"   Please check the error messages above and retry\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Review the generated features for data quality\")\n",
    "print(f\"   2. Use these feature tables for ML model training\")\n",
    "print(f\"   3. Set up scheduled jobs for regular feature updates\")\n",
    "print(f\"   4. Consider adding more advanced features (technical indicators, etc.)\")\n",
    "print(f\"   5. Implement feature monitoring and alerting\")\n",
    "\n",
    "print(f\"\\n‚ú® Feature engineering notebook completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
