{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8557d5bf-a5ac-45f8-b879-ba8a9460d713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering Validation\n",
    "\n",
    "This notebook validates the output of the feature engineering process for financial data. It performs comprehensive data quality checks including:\n",
    "\n",
    "- **Schema Validation**: Ensures all expected columns are present\n",
    "- **Null Value Analysis**: Identifies missing data and completeness metrics\n",
    "- **Outlier Detection**: Detects extreme values in daily returns and other features\n",
    "- **Data Quality Summary**: Provides overall assessment of feature tables\n",
    "\n",
    "## Expected Validation Results:\n",
    "- âœ… No missing feature columns\n",
    "- âœ… Null counts minimal (< 5% of total records)\n",
    "- âœ… Outlier count reasonable (< 1% extreme outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32631ca-e4f2-4ab5-a9d7-9831594514cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import PySpark SQL functions and other necessary libraries for data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c449a219-6f93-4a4a-b586-c27b8677c229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"Validation started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2976144-a2e6-4c6a-86d0-bfa4b846c69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Define Stock Symbols and Table Names\n",
    "\n",
    "Set up the list of stock symbols to validate and construct corresponding table names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b824df5-3039-47ea-96d7-17a79da93432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define symbols to validate\n",
    "symbols = [\"AAPL\", \"MSFT\"]\n",
    "\n",
    "# Expected feature columns\n",
    "expected_columns = [\n",
    "    \"ticker\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"daily_return\", \"moving_avg_7\", \"moving_avg_30\", \"volatility_7\", \n",
    "    \"momentum\", \"feature_timestamp\"\n",
    "]\n",
    "\n",
    "# Create table mapping\n",
    "table_mapping = {}\n",
    "for symbol in symbols:\n",
    "    table_name = f\"main.finance.features_{symbol.lower()}\"\n",
    "    table_mapping[symbol] = table_name\n",
    "\n",
    "print(\"ğŸ¯ Validation Configuration:\")\n",
    "print(f\"   Symbols to validate: {', '.join(symbols)}\")\n",
    "print(f\"   Expected columns: {len(expected_columns)}\")\n",
    "print(f\"   Table mapping:\")\n",
    "for symbol, table in table_mapping.items():\n",
    "    print(f\"     {symbol} â†’ {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca7be8a-50ac-450a-9045-3f7fed873b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Load and Validate Feature Tables\n",
    "\n",
    "Load each feature table using Spark and display the schema to verify column structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44d3724-0696-44f5-ac1c-e959b3cdad9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and validate each feature table\n",
    "dataframes = {}\n",
    "validation_results = {}\n",
    "\n",
    "print(\"ğŸ“Š Loading Feature Tables and Validating Schemas\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for symbol in symbols:\n",
    "    table_name = table_mapping[symbol]\n",
    "    print(f\"\\nğŸ·ï¸ Validating {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists\n",
    "        if spark.catalog.tableExists(table_name):\n",
    "            # Load the table\n",
    "            df = spark.table(table_name)\n",
    "            dataframes[symbol] = df\n",
    "            \n",
    "            print(f\"âœ… Table exists: {table_name}\")\n",
    "            print(f\"ğŸ“Š Row count: {df.count():,}\")\n",
    "            print(f\"ğŸ“‹ Column count: {len(df.columns)}\")\n",
    "            \n",
    "            # Display schema\n",
    "            print(\"ğŸ“ Table Schema:\")\n",
    "            df.printSchema()\n",
    "            \n",
    "            # Initialize validation results\n",
    "            validation_results[symbol] = {\n",
    "                'table_exists': True,\n",
    "                'row_count': df.count(),\n",
    "                'column_count': len(df.columns),\n",
    "                'columns': df.columns\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ Table does not exist: {table_name}\")\n",
    "            validation_results[symbol] = {\n",
    "                'table_exists': False,\n",
    "                'error': f\"Table {table_name} not found\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error accessing table {table_name}: {str(e)}\")\n",
    "        validation_results[symbol] = {\n",
    "            'table_exists': False,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76af267-e239-473c-af31-227bc7edb6bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Check for Missing Columns\n",
    "\n",
    "Validate that all expected feature columns are present in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d502e1-ce82-4451-a71d-ea97a4d45093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing columns in each table\n",
    "print(\"ğŸ” Column Validation Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for symbol in symbols:\n",
    "    if symbol in dataframes:\n",
    "        df = dataframes[symbol]\n",
    "        actual_columns = set(df.columns)\n",
    "        expected_columns_set = set(expected_columns)\n",
    "        \n",
    "        print(f\"\\nğŸ·ï¸ {symbol} Column Analysis:\")\n",
    "        print(f\"   Expected columns: {len(expected_columns_set)}\")\n",
    "        print(f\"   Actual columns: {len(actual_columns)}\")\n",
    "        \n",
    "        # Find missing columns\n",
    "        missing_columns = expected_columns_set - actual_columns\n",
    "        extra_columns = actual_columns - expected_columns_set\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"   âŒ Missing columns: {', '.join(missing_columns)}\")\n",
    "            validation_results[symbol]['missing_columns'] = list(missing_columns)\n",
    "        else:\n",
    "            print(f\"   âœ… All expected columns present\")\n",
    "            validation_results[symbol]['missing_columns'] = []\n",
    "        \n",
    "        if extra_columns:\n",
    "            print(f\"   âš ï¸ Extra columns: {', '.join(extra_columns)}\")\n",
    "            validation_results[symbol]['extra_columns'] = list(extra_columns)\n",
    "        else:\n",
    "            validation_results[symbol]['extra_columns'] = []\n",
    "            \n",
    "        # Overall column validation status\n",
    "        column_validation_passed = len(missing_columns) == 0\n",
    "        validation_results[symbol]['column_validation_passed'] = column_validation_passed\n",
    "        \n",
    "        status = \"âœ… PASSED\" if column_validation_passed else \"âŒ FAILED\"\n",
    "        print(f\"   ğŸ“Š Column Validation: {status}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nğŸ·ï¸ {symbol}: âŒ Table not available for column validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42b4989-300e-474b-8549-bbc05c28b246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Analyze Null Value Counts\n",
    "\n",
    "Count null values in each column to ensure data completeness and identify potential data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95949f4e-ba61-46ad-9215-61121b6bc125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze null values for each symbol\n",
    "print(\"ğŸ” Null Value Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for symbol in symbols:\n",
    "    if symbol in dataframes:\n",
    "        df = dataframes[symbol]\n",
    "        table_name = table_mapping[symbol]\n",
    "        \n",
    "        print(f\"\\nğŸ·ï¸ {symbol} Null Analysis ({table_name}):\")\n",
    "        \n",
    "        # Check for nulls using the provided pattern\n",
    "        null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "        \n",
    "        # Display null counts (using display for Databricks)\n",
    "        print(\"ğŸ“Š Null Counts by Column:\")\n",
    "        display(null_counts)\n",
    "        \n",
    "        # Collect null count data for analysis\n",
    "        null_data = null_counts.collect()[0].asDict()\n",
    "        total_rows = df.count()\n",
    "        \n",
    "        # Analyze null percentages\n",
    "        null_analysis = {}\n",
    "        high_null_columns = []\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Null Percentage Analysis (Total rows: {total_rows:,}):\")\n",
    "        for col, null_count in null_data.items():\n",
    "            null_percentage = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "            null_analysis[col] = {\n",
    "                'count': null_count,\n",
    "                'percentage': null_percentage\n",
    "            }\n",
    "            \n",
    "            # Flag columns with high null percentages\n",
    "            if null_percentage > 5.0:  # More than 5% nulls\n",
    "                high_null_columns.append(col)\n",
    "                status = \"âŒ\"\n",
    "            elif null_percentage > 1.0:  # More than 1% nulls\n",
    "                status = \"âš ï¸\"\n",
    "            else:\n",
    "                status = \"âœ…\"\n",
    "            \n",
    "            print(f\"   {status} {col}: {null_count:,} nulls ({null_percentage:.2f}%)\")\n",
    "        \n",
    "        # Store results\n",
    "        validation_results[symbol]['null_analysis'] = null_analysis\n",
    "        validation_results[symbol]['high_null_columns'] = high_null_columns\n",
    "        validation_results[symbol]['null_validation_passed'] = len(high_null_columns) == 0\n",
    "        \n",
    "        if high_null_columns:\n",
    "            print(f\"   âš ï¸ Columns with high null rates (>5%): {', '.join(high_null_columns)}\")\n",
    "        else:\n",
    "            print(f\"   âœ… All columns have acceptable null rates (<5%)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"\\nğŸ·ï¸ {symbol}: âŒ Table not available for null analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979ad1d3-d551-4262-a49c-0e3d222711be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Detect and Count Outliers\n",
    "\n",
    "Identify outliers in daily returns using threshold filters and count occurrences for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a9a884-f49d-4df9-a90b-c86d5599e225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detect outliers in daily returns and other features\n",
    "print(\"ğŸ” Outlier Detection Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define outlier thresholds\n",
    "outlier_thresholds = {\n",
    "    'daily_return': {'extreme': 0.2, 'moderate': 0.1},  # Â±20% and Â±10%\n",
    "    'volatility_7': {'extreme': 0.5, 'moderate': 0.3},   # High volatility thresholds\n",
    "    'momentum': {'extreme': 2.0, 'moderate': 1.5}        # Momentum ratio thresholds\n",
    "}\n",
    "\n",
    "for symbol in symbols:\n",
    "    if symbol in dataframes:\n",
    "        df = dataframes[symbol]\n",
    "        table_name = table_mapping[symbol]\n",
    "        \n",
    "        print(f\"\\nğŸ·ï¸ {symbol} Outlier Analysis ({table_name}):\")\n",
    "        \n",
    "        outlier_results = {}\n",
    "        \n",
    "        # Analyze each feature for outliers\n",
    "        for feature, thresholds in outlier_thresholds.items():\n",
    "            if feature in df.columns:\n",
    "                extreme_threshold = thresholds['extreme']\n",
    "                moderate_threshold = thresholds['moderate']\n",
    "                \n",
    "                # Count extreme outliers (using the original pattern)\n",
    "                if feature == 'daily_return':\n",
    "                    extreme_outliers = df.filter(\n",
    "                        (F.col(feature) < -extreme_threshold) | \n",
    "                        (F.col(feature) > extreme_threshold)\n",
    "                    )\n",
    "                    moderate_outliers = df.filter(\n",
    "                        (F.col(feature) < -moderate_threshold) | \n",
    "                        (F.col(feature) > moderate_threshold)\n",
    "                    )\n",
    "                else:\n",
    "                    # For other features, only check upper bounds\n",
    "                    extreme_outliers = df.filter(F.col(feature) > extreme_threshold)\n",
    "                    moderate_outliers = df.filter(F.col(feature) > moderate_threshold)\n",
    "                \n",
    "                extreme_count = extreme_outliers.count()\n",
    "                moderate_count = moderate_outliers.count()\n",
    "                total_rows = df.count()\n",
    "                \n",
    "                extreme_percentage = (extreme_count / total_rows * 100) if total_rows > 0 else 0\n",
    "                moderate_percentage = (moderate_count / total_rows * 100) if total_rows > 0 else 0\n",
    "                \n",
    "                print(f\"   ğŸ“Š {feature}:\")\n",
    "                print(f\"      Extreme outliers (Â±{extreme_threshold}): {extreme_count:,} ({extreme_percentage:.2f}%)\")\n",
    "                print(f\"      Moderate outliers (Â±{moderate_threshold}): {moderate_count:,} ({moderate_percentage:.2f}%)\")\n",
    "                \n",
    "                # Store results\n",
    "                outlier_results[feature] = {\n",
    "                    'extreme_count': extreme_count,\n",
    "                    'extreme_percentage': extreme_percentage,\n",
    "                    'moderate_count': moderate_count,\n",
    "                    'moderate_percentage': moderate_percentage\n",
    "                }\n",
    "                \n",
    "                # Flag if outlier rate is too high (>1% extreme outliers)\n",
    "                if extreme_percentage > 1.0:\n",
    "                    print(f\"      âš ï¸ High extreme outlier rate: {extreme_percentage:.2f}%\")\n",
    "                else:\n",
    "                    print(f\"      âœ… Acceptable outlier rate: {extreme_percentage:.2f}%\")\n",
    "        \n",
    "        # Original pattern for daily_return outliers\n",
    "        outliers = df.filter((F.col(\"daily_return\") < -0.2) | (F.col(\"daily_return\") > 0.2))\n",
    "        outlier_count = outliers.count()\n",
    "        print(f\"\\nğŸ“ˆ Outliers in {table_name}: {outlier_count}\")\n",
    "        \n",
    "        # Display sample outliers if any exist\n",
    "        if outlier_count > 0 and outlier_count <= 20:\n",
    "            print(f\"ğŸ“‹ Sample outlier records:\")\n",
    "            outliers.select(\"date\", \"ticker\", \"daily_return\", \"close\").orderBy(F.desc(\"daily_return\")).show(10)\n",
    "        \n",
    "        # Store validation results\n",
    "        validation_results[symbol]['outlier_analysis'] = outlier_results\n",
    "        validation_results[symbol]['total_extreme_outliers'] = outlier_count\n",
    "        validation_results[symbol]['outlier_validation_passed'] = (outlier_count / df.count() * 100) < 1.0  # <1% outliers\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nğŸ·ï¸ {symbol}: âŒ Table not available for outlier analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c3af40-d498-4841-a49f-1ff5ecf4b03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Generate Validation Summary Report\n",
    "\n",
    "Create a comprehensive summary report showing validation results including missing columns, null counts, and outlier statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d888a7-e26b-4bee-a334-5dffd3a8f75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate comprehensive validation summary report\n",
    "print(\"ğŸ“‹ FEATURE VALIDATION SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Validation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Symbols validated: {', '.join(symbols)}\")\n",
    "\n",
    "# Overall validation status\n",
    "all_validations_passed = True\n",
    "summary_stats = {\n",
    "    'tables_validated': 0,\n",
    "    'tables_passed': 0,\n",
    "    'total_rows': 0,\n",
    "    'total_columns': 0,\n",
    "    'total_outliers': 0\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š VALIDATION RESULTS BY SYMBOL:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for symbol in symbols:\n",
    "    if symbol in validation_results and validation_results[symbol].get('table_exists', False):\n",
    "        result = validation_results[symbol]\n",
    "        \n",
    "        # Individual validation checks\n",
    "        table_exists = result.get('table_exists', False)\n",
    "        column_validation = result.get('column_validation_passed', False)\n",
    "        null_validation = result.get('null_validation_passed', False) \n",
    "        outlier_validation = result.get('outlier_validation_passed', False)\n",
    "        \n",
    "        # Overall symbol validation\n",
    "        symbol_passed = all([table_exists, column_validation, null_validation, outlier_validation])\n",
    "        status_emoji = \"âœ…\" if symbol_passed else \"âŒ\"\n",
    "        \n",
    "        print(f\"\\n{status_emoji} {symbol} ({table_mapping[symbol]}):\")\n",
    "        print(f\"   ğŸ“Š Rows: {result.get('row_count', 0):,}\")\n",
    "        print(f\"   ğŸ“‹ Columns: {result.get('column_count', 0)}\")\n",
    "        print(f\"   ğŸ” Schema: {'âœ… PASS' if column_validation else 'âŒ FAIL'}\")\n",
    "        print(f\"   ğŸ”¢ Nulls: {'âœ… PASS' if null_validation else 'âŒ FAIL'}\")\n",
    "        print(f\"   ğŸ“ˆ Outliers: {'âœ… PASS' if outlier_validation else 'âŒ FAIL'}\")\n",
    "        \n",
    "        # Detailed issues if any\n",
    "        if result.get('missing_columns'):\n",
    "            print(f\"      âš ï¸ Missing columns: {', '.join(result['missing_columns'])}\")\n",
    "        if result.get('high_null_columns'):\n",
    "            print(f\"      âš ï¸ High null columns: {', '.join(result['high_null_columns'])}\")\n",
    "        if result.get('total_extreme_outliers', 0) > 0:\n",
    "            outlier_pct = (result['total_extreme_outliers'] / result.get('row_count', 1)) * 100\n",
    "            print(f\"      âš ï¸ Extreme outliers: {result['total_extreme_outliers']} ({outlier_pct:.2f}%)\")\n",
    "        \n",
    "        # Update summary stats\n",
    "        summary_stats['tables_validated'] += 1\n",
    "        if symbol_passed:\n",
    "            summary_stats['tables_passed'] += 1\n",
    "        summary_stats['total_rows'] += result.get('row_count', 0)\n",
    "        summary_stats['total_columns'] += result.get('column_count', 0)\n",
    "        summary_stats['total_outliers'] += result.get('total_extreme_outliers', 0)\n",
    "        \n",
    "        if not symbol_passed:\n",
    "            all_validations_passed = False\n",
    "            \n",
    "    else:\n",
    "        print(f\"\\nâŒ {symbol}: Table not found or inaccessible\")\n",
    "        all_validations_passed = False\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nğŸ¯ OVERALL VALIDATION SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"ğŸ“Š Tables validated: {summary_stats['tables_validated']}\")\n",
    "print(f\"âœ… Tables passed: {summary_stats['tables_passed']}\")\n",
    "print(f\"âŒ Tables failed: {summary_stats['tables_validated'] - summary_stats['tables_passed']}\")\n",
    "print(f\"ğŸ“ˆ Total data rows: {summary_stats['total_rows']:,}\")\n",
    "print(f\"ğŸ“‹ Average columns per table: {summary_stats['total_columns'] / max(summary_stats['tables_validated'], 1):.1f}\")\n",
    "print(f\"âš ï¸ Total extreme outliers: {summary_stats['total_outliers']:,}\")\n",
    "\n",
    "# Overall status\n",
    "if all_validations_passed:\n",
    "    print(f\"\\nğŸ‰ VALIDATION RESULT: âœ… ALL CHECKS PASSED\")\n",
    "    print(f\"   â€¢ âœ… No missing feature columns\")\n",
    "    print(f\"   â€¢ âœ… Null counts minimal\")  \n",
    "    print(f\"   â€¢ âœ… Outlier count reasonable\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ VALIDATION RESULT: âŒ SOME CHECKS FAILED\")\n",
    "    print(f\"   Please review the detailed results above\")\n",
    "\n",
    "print(f\"\\nğŸ“ Next Steps:\")\n",
    "if all_validations_passed:\n",
    "    print(f\"   1. âœ… Feature tables are ready for ML model training\")\n",
    "    print(f\"   2. ğŸš€ Proceed with model development workflows\")\n",
    "    print(f\"   3. ğŸ“Š Set up monitoring for ongoing data quality\")\n",
    "else:\n",
    "    print(f\"   1. ğŸ”§ Address the validation failures identified above\")\n",
    "    print(f\"   2. ğŸ”„ Re-run feature engineering if necessary\")\n",
    "    print(f\"   3. ğŸ” Investigate data quality issues\")\n",
    "\n",
    "print(f\"\\nâœ¨ Validation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_validate_features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
