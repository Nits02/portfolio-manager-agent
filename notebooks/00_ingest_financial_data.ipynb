{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f2f352",
   "metadata": {},
   "source": [
    "# Financial Data Ingestion to Delta Lake\n",
    "\n",
    "This notebook downloads financial data from Yahoo Finance and ingests it into our Delta Lake bronze layer. \n",
    "The process includes:\n",
    "1. Downloading historical price data for selected stock symbols\n",
    "2. Transforming and validating the data\n",
    "3. Writing to Delta Lake with proper optimization\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, LongType\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with Delta Lake support\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"FinanceDataIngestion\")\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31312de5",
   "metadata": {},
   "source": [
    "## Configure Schema and Error Handling\n",
    "\n",
    "Define the schema for our financial data and set up error handling for the Yahoo Finance API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for financial data\n",
    "schema = StructType([\n",
    "    StructField(\"ticker\", StringType(), False),\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"adj_close\", DoubleType(), True),\n",
    "    StructField(\"volume\", LongType(), True),\n",
    "    StructField(\"ingestion_timestamp\", DateType(), False)\n",
    "])\n",
    "\n",
    "# Error handling class\n",
    "class DataIngestionError(Exception):\n",
    "    \"\"\"Custom exception for data ingestion errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Initialize ingestion stats dictionary\n",
    "ingestion_stats = {\n",
    "    \"successful_tickers\": [],\n",
    "    \"failed_tickers\": [],\n",
    "    \"total_rows\": 0,\n",
    "    \"start_time\": None,\n",
    "    \"end_time\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b3ca5",
   "metadata": {},
   "source": [
    "## Define Target Symbols and Date Range\n",
    "\n",
    "Specify the list of stock symbols to download and the time period for historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a630a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target stock symbols\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META']\n",
    "\n",
    "# Set date range (default: 1 year of data)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "print(f\"Configured to download data for {len(tickers)} symbols:\")\n",
    "print(f\"Symbols: {', '.join(tickers)}\")\n",
    "print(f\"Date range: {start_date.date()} to {end_date.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0cc1c",
   "metadata": {},
   "source": [
    "## Download and Transform Financial Data\n",
    "\n",
    "Download historical price data for all symbols using yfinance API and transform it into the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process data\n",
    "ingestion_stats[\"start_time\"] = datetime.now()\n",
    "all_data = []\n",
    "\n",
    "try:\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            logger.info(f\"Downloading data for {ticker}...\")\n",
    "            yf_ticker = yf.Ticker(ticker)\n",
    "            hist = yf_ticker.history(start=start_date, end=end_date)\n",
    "            \n",
    "            if hist.empty:\n",
    "                raise DataIngestionError(f\"No data available for {ticker}\")\n",
    "            \n",
    "            # Handle timezone and date conversion\n",
    "            if hasattr(hist.index, 'tz_localize'):\n",
    "                hist.index = hist.index.tz_localize(None)\n",
    "            hist.reset_index(inplace=True)\n",
    "            \n",
    "            # Convert date to proper datetime format\n",
    "            hist['Date'] = pd.to_datetime(hist['Date'])\n",
    "            hist['ticker'] = ticker\n",
    "            \n",
    "            # Rename columns immediately after download to ensure consistency\n",
    "            hist.columns = hist.columns.str.lower().str.replace(' ', '_')\n",
    "            \n",
    "            all_data.append(hist)\n",
    "            logger.info(f\"Successfully downloaded {len(hist)} rows for {ticker}\")\n",
    "            ingestion_stats[\"successful_tickers\"].append(ticker)\n",
    "            \n",
    "        except Exception as exc:\n",
    "            logger.error(f\"Failed to download data for {ticker}: {str(exc)}\")\n",
    "            ingestion_stats[\"failed_tickers\"].append(ticker)\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        raise DataIngestionError(\"No data downloaded for any ticker\")\n",
    "\n",
    "    # Combine all data and add ingestion timestamp\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    combined_data['ingestion_timestamp'] = pd.to_datetime(datetime.now().date())\n",
    "    \n",
    "    # Verify column names after combining\n",
    "    logger.info(f\"Available columns: {list(combined_data.columns)}\")\n",
    "\n",
    "    ingestion_stats[\"total_rows\"] = len(combined_data)\n",
    "    logger.info(f\"Successfully combined data: {len(combined_data)} total rows\")\n",
    "    \n",
    "except Exception as exc:\n",
    "    logger.error(f\"Data download failed: {str(exc)}\")\n",
    "    raise DataIngestionError(f\"Data download failed: {str(exc)}\")\n",
    "finally:\n",
    "    ingestion_stats[\"end_time\"] = datetime.now()\n",
    "\n",
    "# Display ingestion summary\n",
    "duration = ingestion_stats[\"end_time\"] - ingestion_stats[\"start_time\"]\n",
    "print(\"\\n=== Ingestion Summary ===\")\n",
    "print(f\"Duration: {duration.total_seconds():.2f} seconds\")\n",
    "print(f\"Total rows: {ingestion_stats['total_rows']}\")\n",
    "print(f\"Successful tickers: {', '.join(ingestion_stats['successful_tickers'])}\")\n",
    "if ingestion_stats['failed_tickers']:\n",
    "    print(f\"Failed tickers: {', '.join(ingestion_stats['failed_tickers'])}\")\n",
    "print(\"=====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cbe6a",
   "metadata": {},
   "source": [
    "## Write to Delta Lake Bronze Layer\n",
    "\n",
    "Write the transformed data to Delta Lake with proper optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524164fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the catalog\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS finance_catalog\")\n",
    "spark.sql(\"USE CATALOG finance_catalog\")\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "spark.sql(\"USE bronze\")\n",
    "\n",
    "try:\n",
    "    if 'adj_close' not in combined_data.columns:\n",
    "        combined_data['adj_close'] = combined_data['close']\n",
    "    \n",
    "    selected_columns = ['ticker', 'date', 'open', 'high', 'low', 'close', \n",
    "                       'adj_close', 'volume', 'ingestion_timestamp']\n",
    "    combined_data_filtered = combined_data[selected_columns]\n",
    "    \n",
    "    spark_df = spark.createDataFrame(combined_data_filtered)\n",
    "    \n",
    "    spark_df = spark_df.select(\n",
    "        spark_df.ticker.cast(StringType()),\n",
    "        spark_df.date.cast(DateType()),\n",
    "        spark_df.open.cast(DoubleType()),\n",
    "        spark_df.high.cast(DoubleType()),\n",
    "        spark_df.low.cast(DoubleType()),\n",
    "        spark_df.close.cast(DoubleType()),\n",
    "        spark_df.adj_close.cast(DoubleType()),\n",
    "        spark_df.volume.cast(LongType()),\n",
    "        spark_df.ingestion_timestamp.cast(DateType())\n",
    "    )\n",
    "    \n",
    "    table_name = \"prices\"\n",
    "    (spark_df.write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .option(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
    "     .option(\"delta.autoOptimize.autoCompact\", \"true\")\n",
    "     .saveAsTable(f\"finance_catalog.bronze.{table_name}\"))\n",
    "    \n",
    "    row_count = spark_df.count()\n",
    "    print(f\"\\nSuccessfully wrote {row_count} rows to finance_catalog.bronze.{table_name}\")\n",
    "    \n",
    "except Exception as exc:\n",
    "    logger.error(\"Failed to write to Delta table\", exc_info=True)\n",
    "    raise DataIngestionError(f\"Delta table write failed: {str(exc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bafaa",
   "metadata": {},
   "source": [
    "## Initial Data Validation\n",
    "\n",
    "Perform basic validation of the ingested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the table and create a view\n",
    "prices_df = spark.table(\"finance_catalog.bronze.prices\")\n",
    "prices_df.createOrReplaceTempView(\"prices_view\")\n",
    "\n",
    "# Get basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"Total records: {prices_df.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "prices_df.printSchema()\n",
    "\n",
    "# Check record counts by symbol\n",
    "symbol_counts = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ticker,\n",
    "        COUNT(*) as record_count\n",
    "    FROM prices_view\n",
    "    GROUP BY ticker\n",
    "    ORDER BY record_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRecord counts by symbol:\")\n",
    "display(symbol_counts)\n",
    "\n",
    "# Show sample of recent data\n",
    "print(\"\\nSample of recent data:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM prices_view\n",
    "    ORDER BY date DESC\n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
