{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f2f352",
   "metadata": {},
   "source": [
    "# Financial Data Ingestion to Delta Lake\n",
    "\n",
    "This notebook downloads financial data from Yahoo Finance and ingests it into our Delta Lake bronze layer. \n",
    "The process includes:\n",
    "1. Downloading historical price data for selected stock symbols\n",
    "2. Transforming and validating the data\n",
    "3. Writing to Delta Lake with proper optimization\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add the project root to Python path to import our agent\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.agents.ingest_agent import DataIngestionAgent, DataIngestionError\n",
    "\n",
    "# Initialize the data ingestion agent\n",
    "agent = DataIngestionAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31312de5",
   "metadata": {},
   "source": [
    "## Configure Target Symbols and Date Range\n",
    "\n",
    "Define the list of stock symbols to download and the time period for historical data. The schema and error handling are already configured in the DataIngestionAgent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for financial data\n",
    "schema = StructType([\n",
    "    StructField(\"ticker\", StringType(), False),\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"adj_close\", DoubleType(), True),\n",
    "    StructField(\"volume\", LongType(), True),\n",
    "    StructField(\"ingestion_timestamp\", DateType(), False)\n",
    "])\n",
    "\n",
    "# Error handling class\n",
    "class DataIngestionError(Exception):\n",
    "    \"\"\"Custom exception for data ingestion errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Initialize ingestion stats dictionary\n",
    "ingestion_stats = {\n",
    "    \"successful_tickers\": [],\n",
    "    \"failed_tickers\": [],\n",
    "    \"total_rows\": 0,\n",
    "    \"start_time\": None,\n",
    "    \"end_time\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b3ca5",
   "metadata": {},
   "source": [
    "## Define Target Symbols and Date Range\n",
    "\n",
    "Specify the list of stock symbols to download and the time period for historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a630a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target stock symbols\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META']\n",
    "\n",
    "# Set date range (default: 1 year of data)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "print(f\"Configured to download data for {len(tickers)} symbols:\")\n",
    "print(f\"Symbols: {', '.join(tickers)}\")\n",
    "print(f\"Date range: {start_date.date()} to {end_date.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0cc1c",
   "metadata": {},
   "source": [
    "## Download and Ingest Financial Data\n",
    "\n",
    "Use the DataIngestionAgent to download historical price data for the selected symbols and ingest it into Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download price data using the agent\n",
    "try:\n",
    "    # Download the data\n",
    "    price_data = agent.download_price_data(\n",
    "        tickers=tickers,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date\n",
    "    )\n",
    "    \n",
    "    # Convert date column to datetime before ingestion\n",
    "    price_data['date'] = pd.to_datetime(price_data['date'])\n",
    "    \n",
    "    # Ingest to Delta Lake\n",
    "    agent.ingest_to_delta(price_data)\n",
    "    \n",
    "except DataIngestionError as e:\n",
    "    print(f\"Data ingestion failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cbe6a",
   "metadata": {},
   "source": [
    "## Write to Delta Lake Bronze Layer\n",
    "\n",
    "Write the transformed data to Delta Lake with proper optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524164fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the catalog\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS finance_catalog\")\n",
    "spark.sql(\"USE CATALOG finance_catalog\")\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "spark.sql(\"USE bronze\")\n",
    "\n",
    "try:\n",
    "    if 'adj_close' not in combined_data.columns:\n",
    "        combined_data['adj_close'] = combined_data['close']\n",
    "    \n",
    "    selected_columns = ['ticker', 'date', 'open', 'high', 'low', 'close', \n",
    "                       'adj_close', 'volume', 'ingestion_timestamp']\n",
    "    combined_data_filtered = combined_data[selected_columns]\n",
    "    \n",
    "    spark_df = spark.createDataFrame(combined_data_filtered)\n",
    "    \n",
    "    spark_df = spark_df.select(\n",
    "        spark_df.ticker.cast(StringType()),\n",
    "        spark_df.date.cast(DateType()),\n",
    "        spark_df.open.cast(DoubleType()),\n",
    "        spark_df.high.cast(DoubleType()),\n",
    "        spark_df.low.cast(DoubleType()),\n",
    "        spark_df.close.cast(DoubleType()),\n",
    "        spark_df.adj_close.cast(DoubleType()),\n",
    "        spark_df.volume.cast(LongType()),\n",
    "        spark_df.ingestion_timestamp.cast(DateType())\n",
    "    )\n",
    "    \n",
    "    table_name = \"prices\"\n",
    "    (spark_df.write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .option(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
    "     .option(\"delta.autoOptimize.autoCompact\", \"true\")\n",
    "     .saveAsTable(f\"finance_catalog.bronze.{table_name}\"))\n",
    "    \n",
    "    row_count = spark_df.count()\n",
    "    print(f\"\\nSuccessfully wrote {row_count} rows to finance_catalog.bronze.{table_name}\")\n",
    "    \n",
    "except Exception as exc:\n",
    "    logger.error(\"Failed to write to Delta table\", exc_info=True)\n",
    "    raise DataIngestionError(f\"Delta table write failed: {str(exc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bafaa",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "\n",
    "Perform basic validation of the ingested data using Spark SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the table and create a view\n",
    "spark = agent.spark\n",
    "prices_df = spark.table(f\"{agent.catalog}.{agent.database}.prices\")\n",
    "prices_df.createOrReplaceTempView(\"prices_view\")\n",
    "\n",
    "# Get basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"Total records: {prices_df.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "prices_df.printSchema()\n",
    "\n",
    "# Check record counts by symbol\n",
    "symbol_counts = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ticker,\n",
    "        COUNT(*) as record_count,\n",
    "        MIN(date) as first_date,\n",
    "        MAX(date) as last_date\n",
    "    FROM prices_view\n",
    "    GROUP BY ticker\n",
    "    ORDER BY record_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRecord counts by symbol:\")\n",
    "display(symbol_counts)\n",
    "\n",
    "# Show sample of recent data\n",
    "print(\"\\nSample of recent data:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM prices_view\n",
    "    ORDER BY date DESC, ticker\n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
