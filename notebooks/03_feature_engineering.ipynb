{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d158cd1f-94f8-47e9-82ed-6c20e5522644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering for Portfolio Management\n",
    "\n",
    "This notebook demonstrates the FeatureEngineeringAgent functionality:\n",
    "1. Import and initialize the FeatureEngineeringAgent\n",
    "2. Process sample tickers (AAPL, MSFT) to create financial features\n",
    "3. Validate output and display sample data from Unity Catalog tables\n",
    "4. Analyze the generated features for ML readiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4870e6e-96c2-42c3-a186-1ad0361cb1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and initialize Spark session if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2d77ea-0027-4147-8b51-e33f7440ff60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom FeatureEngineeringAgent\n",
    "from agents.feature_engineering_agent import FeatureEngineeringAgent, FeatureEngineeringError\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Python path includes: {[p for p in sys.path if 'src' in p]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f78f67-2428-4def-a10b-cfbe561f29e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Initialize Spark Session and FeatureEngineeringAgent\n",
    "\n",
    "Create Spark session and initialize the feature engineering agent with Unity Catalog configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa789fb6-e346-425a-908a-d204c734b3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session (if not already available in Databricks)\n",
    "# In Databricks, spark session is usually pre-configured\n",
    "try:\n",
    "    # Check if spark session already exists (common in Databricks)\n",
    "    spark_session = spark\n",
    "    print(\"‚úÖ Using existing Spark session from Databricks\")\n",
    "except NameError:\n",
    "    # Create new Spark session if not in Databricks environment\n",
    "    spark_session = SparkSession.builder \\\n",
    "        .appName(\"FeatureEngineering\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"‚úÖ Created new Spark session\")\n",
    "\n",
    "print(f\"Spark version: {spark_session.version}\")\n",
    "# print(f\"Spark application: {spark_session.sparkContext.getConf().getAppName()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642e737a-22bb-4262-a9e3-0130fca90306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize FeatureEngineeringAgent\n",
    "print(\"üöÄ Initializing FeatureEngineeringAgent...\")\n",
    "\n",
    "# Create agent with Unity Catalog configuration\n",
    "feature_agent = FeatureEngineeringAgent(catalog=\"portfolio_catalog\", schema=\"portfolio_schema\")\n",
    "\n",
    "print(f\"‚úÖ FeatureEngineeringAgent initialized:\")\n",
    "print(f\"   - Catalog: {feature_agent.catalog}\")\n",
    "print(f\"   - Schema: {feature_agent.schema}\")\n",
    "print(f\"   - Target namespace: {feature_agent.catalog}.{feature_agent.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e4b3a8-56f6-451f-8318-0900fa8aead2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.5. Setup Unity Catalog Schema\n",
    "\n",
    "Create the required catalog and schema if they don't exist. This is required for Unity Catalog environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3e728e-7a0a-44c1-902b-e0957e3dcee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup Unity Catalog structure - Create catalog and schema if they don't exist\n",
    "print(\"üîß Setting up Unity Catalog structure...\")\n",
    "\n",
    "def setup_unity_catalog(catalog_name=\"portfolio_catalog\", schema_name=\"portfolio_schema\"):\n",
    "    \"\"\"Setup Unity Catalog and schema structure\"\"\"\n",
    "    try:\n",
    "        # Check and create catalog\n",
    "        print(f\"üìã Checking catalog: {catalog_name}\")\n",
    "        try:\n",
    "            spark_session.sql(f\"USE CATALOG {catalog_name}\")\n",
    "            print(f\"‚úÖ Catalog '{catalog_name}' exists and is accessible\")\n",
    "        except Exception as e:\n",
    "            if \"CATALOG_NOT_FOUND\" in str(e):\n",
    "                print(f\"‚ö†Ô∏è Catalog '{catalog_name}' not found. Attempting to create...\")\n",
    "                try:\n",
    "                    spark_session.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "                    print(f\"‚úÖ Created catalog: {catalog_name}\")\n",
    "                except Exception as create_error:\n",
    "                    print(f\"‚ùå Failed to create catalog: {str(create_error)}\")\n",
    "                    print(\"   You may need METASTORE_ADMIN permissions to create catalogs\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"‚ùå Error accessing catalog: {str(e)}\")\n",
    "                return False\n",
    "        \n",
    "        # Check and create schema\n",
    "        print(f\"üìã Checking schema: {catalog_name}.{schema_name}\")\n",
    "        try:\n",
    "            # Use the catalog first, then check if schema exists\n",
    "            spark_session.sql(f\"USE CATALOG {catalog_name}\")\n",
    "            \n",
    "            # Check if schema exists by listing schemas\n",
    "            schemas_result = spark_session.sql(\"SHOW SCHEMAS\").collect()\n",
    "            \n",
    "            # Debug: Print schema structure to understand the column names\n",
    "            if schemas_result:\n",
    "                print(f\"üîç Debug: Schema result columns: {schemas_result[0].asDict().keys()}\")\n",
    "                \n",
    "            # Try different possible column names for schema information\n",
    "            schema_exists = False\n",
    "            for row in schemas_result:\n",
    "                row_dict = row.asDict()\n",
    "                # Try different possible column names\n",
    "                schema_value = (row_dict.get('namespace') or \n",
    "                              row_dict.get('schemaName') or \n",
    "                              row_dict.get('databaseName') or\n",
    "                              row_dict.get('name') or\n",
    "                              str(row_dict))\n",
    "                \n",
    "                print(f\"üîç Found schema entry: {schema_value}\")\n",
    "                if schema_value == schema_name:\n",
    "                    schema_exists = True\n",
    "                    break\n",
    "            \n",
    "            if schema_exists:\n",
    "                spark_session.sql(f\"USE SCHEMA {schema_name}\")\n",
    "                print(f\"‚úÖ Schema '{catalog_name}.{schema_name}' exists and is accessible\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Schema '{schema_name}' not found. Creating...\")\n",
    "                try:\n",
    "                    spark_session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "                    spark_session.sql(f\"USE SCHEMA {schema_name}\")\n",
    "                    print(f\"‚úÖ Created schema: {catalog_name}.{schema_name}\")\n",
    "                except Exception as create_error:\n",
    "                    print(f\"‚ùå Failed to create schema: {str(create_error)}\")\n",
    "                    return False\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error accessing schema: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Verify setup by showing current catalog and schema\n",
    "        try:\n",
    "            current_catalog = spark_session.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "            current_schema = spark_session.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "            print(f\"üìä Current catalog: {current_catalog}\")\n",
    "            print(f\"üìä Current schema: {current_schema}\")\n",
    "            \n",
    "            # Also show available schemas in the catalog for verification\n",
    "            print(f\"üìã Available schemas in {catalog_name}:\")\n",
    "            try:\n",
    "                schemas_list = spark_session.sql(f\"SHOW SCHEMAS IN {catalog_name}\").collect()\n",
    "                for schema_row in schemas_list:\n",
    "                    schema_dict = schema_row.asDict()\n",
    "                    # Handle different possible column names\n",
    "                    schema_name_found = (schema_dict.get('namespace') or \n",
    "                                       schema_dict.get('schemaName') or \n",
    "                                       schema_dict.get('databaseName') or\n",
    "                                       schema_dict.get('name') or\n",
    "                                       'unknown')\n",
    "                    print(f\"   - {schema_name_found}\")\n",
    "            except Exception as show_error:\n",
    "                print(f\"   Could not list schemas: {str(show_error)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not retrieve current catalog/schema info: {str(e)}\")\n",
    "            # Continue anyway as this is just informational\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to setup Unity Catalog: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run the setup\n",
    "setup_success = setup_unity_catalog(\"portfolio_catalog\", \"portfolio_schema\")\n",
    "\n",
    "if setup_success:\n",
    "    print(\"\\n‚úÖ Unity Catalog setup completed successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Unity Catalog setup failed. Please check permissions and configuration.\")\n",
    "    print(\"   Required permissions: USE CATALOG, CREATE SCHEMA on the catalog\")\n",
    "    print(\"   Alternative: Ask your admin to create 'portfolio_catalog.portfolio_schema' schema for you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654f2aa2-0bbe-420b-bd08-6d163c318126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Check Available Raw Data\n",
    "\n",
    "Before running feature engineering, let's verify that the raw data tables exist in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "287d2f2b-1f67-4bd5-b24a-a5a3749578c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check available tables in multiple possible schemas\n",
    "print(\"üìä Checking available raw data tables...\")\n",
    "\n",
    "# Define possible schema locations where raw data might be stored\n",
    "possible_schemas = [\n",
    "    \"portfolio_catalog.portfolio_schema\",  # Current target schema\n",
    "    \"finance_catalog.bronze\",              # Data ingestion agent schema\n",
    "    \"main.finance\",                        # Legacy schema\n",
    "    \"main.default\"                         # Default schema fallback\n",
    "]\n",
    "\n",
    "raw_data_found = False\n",
    "raw_data_schema = None\n",
    "available_tables = []\n",
    "\n",
    "for schema in possible_schemas:\n",
    "    try:\n",
    "        print(f\"\\nüîç Checking schema: {schema}\")\n",
    "        tables = spark_session.sql(f\"SHOW TABLES IN {schema}\").collect()\n",
    "        \n",
    "        if tables:\n",
    "            print(f\"   üìã Found {len(tables)} tables in {schema}:\")\n",
    "            schema_tables = []\n",
    "            for table in tables:\n",
    "                table_name = table['tableName']\n",
    "                schema_tables.append(table_name)\n",
    "                print(f\"      - {table_name}\")\n",
    "                \n",
    "                # Check if it's a raw data table for our target tickers\n",
    "                if any(ticker.lower() in table_name.lower() for ticker in ['aapl', 'msft']) or 'price' in table_name.lower():\n",
    "                    raw_data_found = True\n",
    "                    raw_data_schema = schema\n",
    "                    available_tables.extend([f\"{schema}.{table_name}\"])\n",
    "                    \n",
    "                    # Show sample data from this table\n",
    "                    print(f\"        üìä Sample data from {table_name}:\")\n",
    "                    try:\n",
    "                        sample_df = spark_session.table(f\"{schema}.{table_name}\")\n",
    "                        \n",
    "                        # Try different possible column names\n",
    "                        possible_columns = ['ticker', 'symbol', 'date', 'close', 'volume']\n",
    "                        available_cols = [col for col in possible_columns if col in sample_df.columns]\n",
    "                        \n",
    "                        if available_cols:\n",
    "                            sample_df.select(*available_cols).limit(3).show()\n",
    "                        else:\n",
    "                            # Just show first few columns if standard ones don't exist\n",
    "                            cols_to_show = sample_df.columns[:5]\n",
    "                            sample_df.select(*cols_to_show).limit(3).show()\n",
    "                            \n",
    "                    except Exception as sample_error:\n",
    "                        print(f\"           ‚ö†Ô∏è Could not show sample: {str(sample_error)}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No tables found in {schema}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error accessing {schema}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if raw_data_found:\n",
    "    print(f\"\\n‚úÖ Raw data found in schema: {raw_data_schema}\")\n",
    "    print(f\"   Available tables: {', '.join(available_tables)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No raw data tables found in any schema!\")\n",
    "    print(f\"   Checked schemas: {', '.join(possible_schemas)}\")\n",
    "    print(f\"   üìù Next steps:\")\n",
    "    print(f\"      1. Run the data ingestion notebook (01_ingest_financial_data.ipynb) first\")\n",
    "    print(f\"      2. Or create sample data in the current schema for testing\")\n",
    "    \n",
    "    # Offer to create sample data for testing\n",
    "    print(f\"\\nüîß Creating sample data for testing purposes...\")\n",
    "    \n",
    "    # Create sample data for AAPL and MSFT\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, LongType\n",
    "    from pyspark.sql.functions import lit\n",
    "    \n",
    "    sample_data = [\n",
    "        (\"AAPL\", \"2024-01-01\", 150.0, 155.0, 148.0, 152.0, 151.5, 1000000),\n",
    "        (\"AAPL\", \"2024-01-02\", 152.0, 158.0, 151.0, 157.0, 156.5, 1100000),\n",
    "        (\"AAPL\", \"2024-01-03\", 157.0, 160.0, 155.0, 159.0, 158.5, 1200000),\n",
    "        (\"MSFT\", \"2024-01-01\", 380.0, 385.0, 378.0, 383.0, 382.5, 800000),\n",
    "        (\"MSFT\", \"2024-01-02\", 383.0, 388.0, 381.0, 386.0, 385.5, 850000),\n",
    "        (\"MSFT\", \"2024-01-03\", 386.0, 390.0, 384.0, 388.0, 387.5, 900000)\n",
    "    ]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"ticker\", StringType(), False),\n",
    "        StructField(\"date\", StringType(), False),\n",
    "        StructField(\"open\", DoubleType(), True),\n",
    "        StructField(\"high\", DoubleType(), True),\n",
    "        StructField(\"low\", DoubleType(), True),\n",
    "        StructField(\"close\", DoubleType(), True),\n",
    "        StructField(\"adj_close\", DoubleType(), True),\n",
    "        StructField(\"volume\", LongType(), True)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        sample_df = spark_session.createDataFrame(sample_data, schema)\n",
    "        sample_df = sample_df.withColumn(\"date\", sample_df.date.cast(DateType()))\n",
    "        sample_df = sample_df.withColumn(\"ingestion_timestamp\", lit(\"2024-01-01\").cast(DateType()))\n",
    "        \n",
    "        # Create the sample table in portfolio_catalog.portfolio_schema\n",
    "        sample_table_name = \"portfolio_catalog.portfolio_schema.sample_prices\"\n",
    "        sample_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(sample_table_name)\n",
    "        \n",
    "        print(f\"   ‚úÖ Created sample table: {sample_table_name}\")\n",
    "        print(f\"   üìä Sample contains {sample_df.count()} rows for AAPL and MSFT\")\n",
    "        \n",
    "        raw_data_found = True\n",
    "        raw_data_schema = \"portfolio_catalog.portfolio_schema\"\n",
    "        available_tables = [sample_table_name]\n",
    "        \n",
    "    except Exception as sample_error:\n",
    "        print(f\"   ‚ùå Failed to create sample data: {str(sample_error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd14a0b-4faa-40d4-863d-b0cb579369e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Run Feature Engineering\n",
    "\n",
    "Process AAPL and MSFT tickers to create financial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3404488-1da9-4e39-9b66-a0ca6b4b1b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define target tickers for feature engineering\n",
    "target_tickers = [\"AAPL\", \"MSFT\"]\n",
    "\n",
    "print(f\"üîß Starting feature engineering for: {', '.join(target_tickers)}\")\n",
    "print(f\"   Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Check if we have raw data available\n",
    "if not raw_data_found:\n",
    "    print(\"‚ùå Cannot proceed: No raw data tables found!\")\n",
    "    print(\"   Please run the data ingestion notebook first or check the previous cell output\")\n",
    "    results = None\n",
    "else:\n",
    "    print(f\"‚úÖ Using raw data from schema: {raw_data_schema}\")\n",
    "    print(f\"   Available tables: {', '.join(available_tables)}\")\n",
    "    \n",
    "    try:\n",
    "        # Since our existing FeatureEngineeringAgent expects a specific table structure,\n",
    "        # let's create features manually using the available data\n",
    "        \n",
    "        results = {\n",
    "            'start_time': datetime.now(),\n",
    "            'processed_tickers': [],\n",
    "            'failed_tickers': [],\n",
    "            'total_features_created': 0\n",
    "        }\n",
    "        \n",
    "        # Find the raw data table (could be 'prices', 'sample_prices', etc.)\n",
    "        raw_table = None\n",
    "        for table_path in available_tables:\n",
    "            if any(name in table_path.lower() for name in ['price', 'market', 'ohlc']):\n",
    "                raw_table = table_path\n",
    "                break\n",
    "        \n",
    "        if not raw_table:\n",
    "            raw_table = available_tables[0]  # Use first available table\n",
    "        \n",
    "        print(f\"üìä Using raw data table: {raw_table}\")\n",
    "        \n",
    "        # Read the raw data\n",
    "        raw_df = spark_session.table(raw_table)\n",
    "        print(f\"   üìà Raw data contains {raw_df.count()} rows\")\n",
    "        print(f\"   üìã Columns: {', '.join(raw_df.columns)}\")\n",
    "        \n",
    "        # Process each ticker\n",
    "        for ticker in target_tickers:\n",
    "            try:\n",
    "                print(f\"\\nüîß Processing {ticker}...\")\n",
    "                \n",
    "                # Filter data for this ticker\n",
    "                ticker_df = raw_df.filter(F.col(\"ticker\") == ticker)\n",
    "                \n",
    "                if ticker_df.count() == 0:\n",
    "                    print(f\"   ‚ö†Ô∏è No data found for {ticker}\")\n",
    "                    results['failed_tickers'].append(ticker)\n",
    "                    continue\n",
    "                \n",
    "                # Create features using window functions\n",
    "                window_spec = Window.partitionBy(\"ticker\").orderBy(\"date\")\n",
    "                \n",
    "                feature_df = ticker_df.select(\n",
    "                    \"ticker\",\n",
    "                    \"date\", \n",
    "                    \"open\",\n",
    "                    \"high\",\n",
    "                    \"low\",\n",
    "                    \"close\",\n",
    "                    \"volume\"\n",
    "                ).withColumn(\n",
    "                    # Daily return\n",
    "                    \"daily_return\",\n",
    "                    (F.col(\"close\") - F.lag(\"close\", 1).over(window_spec)) / F.lag(\"close\", 1).over(window_spec)\n",
    "                ).withColumn(\n",
    "                    # 7-day moving average\n",
    "                    \"moving_avg_7\",\n",
    "                    F.avg(\"close\").over(window_spec.rowsBetween(-6, 0))\n",
    "                ).withColumn(\n",
    "                    # 30-day moving average  \n",
    "                    \"moving_avg_30\",\n",
    "                    F.avg(\"close\").over(window_spec.rowsBetween(-29, 0))\n",
    "                ).withColumn(\n",
    "                    # 7-day volatility (rolling standard deviation of returns)\n",
    "                    \"volatility_7\",\n",
    "                    F.stddev(\"daily_return\").over(window_spec.rowsBetween(-6, 0))\n",
    "                ).withColumn(\n",
    "                    # Momentum (price change over 10 days)\n",
    "                    \"momentum\",\n",
    "                    (F.col(\"close\") - F.lag(\"close\", 10).over(window_spec)) / F.lag(\"close\", 10).over(window_spec)\n",
    "                ).withColumn(\n",
    "                    # Feature timestamp\n",
    "                    \"feature_timestamp\",\n",
    "                    F.lit(datetime.now().date())\n",
    "                )\n",
    "                \n",
    "                # Remove null rows (first few rows won't have complete features)\n",
    "                feature_df = feature_df.filter(F.col(\"daily_return\").isNotNull())\n",
    "                \n",
    "                feature_count = feature_df.count()\n",
    "                print(f\"   ‚úÖ Created {feature_count} feature records for {ticker}\")\n",
    "                \n",
    "                # Save to feature table\n",
    "                feature_table_name = f\"portfolio_catalog.portfolio_schema.features_{ticker}\"\n",
    "                \n",
    "                feature_df.write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .saveAsTable(feature_table_name)\n",
    "                \n",
    "                print(f\"   üíæ Saved features to {feature_table_name}\")\n",
    "                \n",
    "                results['processed_tickers'].append(ticker)\n",
    "                results['total_features_created'] += feature_count\n",
    "                \n",
    "            except Exception as ticker_error:\n",
    "                print(f\"   ‚ùå Failed to process {ticker}: {str(ticker_error)}\")\n",
    "                results['failed_tickers'].append(ticker)\n",
    "                continue\n",
    "        \n",
    "        results['end_time'] = datetime.now()\n",
    "        \n",
    "        if results['processed_tickers']:\n",
    "            print(\"\\n‚úÖ Feature engineering completed!\")\n",
    "            print(f\"   Duration: {(results['end_time'] - results['start_time']).total_seconds():.2f} seconds\")\n",
    "            print(f\"   Total features created: {results['total_features_created']}\")\n",
    "            print(f\"   ‚úÖ Successfully processed: {', '.join(results['processed_tickers'])}\")\n",
    "            \n",
    "            if results['failed_tickers']:\n",
    "                print(f\"   ‚ùå Failed tickers: {', '.join(results['failed_tickers'])}\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Feature engineering failed for all tickers\")\n",
    "            results = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Feature engineering failed: {str(e)}\")\n",
    "        print(\"   Check the logs for detailed error information\")\n",
    "        results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec32a7d-b169-4ace-92ab-14f05840641d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Validate Output Tables\n",
    "\n",
    "Check that the feature tables were created successfully in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261ab34a-3921-4edc-9947-cf9746cbe171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for newly created feature tables\n",
    "print(\"üîç Validating output tables...\")\n",
    "\n",
    "feature_tables = []\n",
    "for ticker in target_tickers:\n",
    "    table_name = f\"portfolio_catalog.portfolio_schema.features_{ticker}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists\n",
    "        table_exists = spark_session.catalog.tableExists(table_name)\n",
    "        \n",
    "        if table_exists:\n",
    "            print(f\"‚úÖ {table_name} exists\")\n",
    "            feature_tables.append(table_name)\n",
    "            \n",
    "            # Get table info\n",
    "            df = spark_session.table(table_name)\n",
    "            row_count = df.count()\n",
    "            col_count = len(df.columns)\n",
    "            \n",
    "            print(f\"   üìä Table stats: {row_count:,} rows, {col_count} columns\")\n",
    "            \n",
    "            # Show column names\n",
    "            print(f\"   üìã Columns: {', '.join(df.columns)}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå {table_name} does not exist\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking {table_name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüìà Total feature tables created: {len(feature_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb30fc4-375d-4360-9215-10eeffabe388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Display Sample Feature Data\n",
    "\n",
    "Show sample data from the generated feature tables to verify quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f046509-316e-4fa1-bc73-c70345fe4db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display sample data from feature tables\n",
    "print(\"üìã Sample Feature Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for table_name in feature_tables:\n",
    "    ticker = table_name.split('_')[-1]  # Extract ticker from table name\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è {ticker} Features ({table_name})\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        df = spark_session.table(table_name)\n",
    "        \n",
    "        # Show recent data (last 5 rows)\n",
    "        print(\"\\nüìÖ Most Recent 5 Records:\")\n",
    "        df.orderBy(F.desc(\"date\")).limit(5).show(truncate=False)\n",
    "        \n",
    "        # Show feature summary statistics\n",
    "        print(\"\\nüìä Feature Statistics:\")\n",
    "        feature_cols = ['daily_return', 'moving_avg_7', 'moving_avg_30', 'volatility_7', 'momentum']\n",
    "        df.select(feature_cols).summary().show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error displaying data for {table_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf73b59-db16-41a1-9a1b-a3a9892b52f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Feature Quality Analysis\n",
    "\n",
    "Analyze the quality and completeness of generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc900581-2e95-420b-8be5-80a1613d6817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze feature quality\n",
    "print(\"üî¨ Feature Quality Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for table_name in feature_tables:\n",
    "    ticker = table_name.split('_')[-1]\n",
    "    \n",
    "    print(f\"\\nüìà Analysis for {ticker}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        df = spark_session.table(table_name)\n",
    "        total_rows = df.count()\n",
    "        \n",
    "        # Check for null values in key features\n",
    "        feature_cols = ['daily_return', 'moving_avg_7', 'moving_avg_30', 'volatility_7', 'momentum']\n",
    "        \n",
    "        print(f\"üìä Data Completeness (out of {total_rows:,} total rows):\")\n",
    "        for col in feature_cols:\n",
    "            null_count = df.filter(F.col(col).isNull()).count()\n",
    "            non_null_count = total_rows - null_count\n",
    "            completeness = (non_null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "            \n",
    "            status = \"‚úÖ\" if completeness >= 95 else \"‚ö†Ô∏è\" if completeness >= 80 else \"‚ùå\"\n",
    "            print(f\"   {status} {col}: {completeness:.1f}% complete ({non_null_count:,} values)\")\n",
    "        \n",
    "        # Check date range\n",
    "        date_stats = df.select(\n",
    "            F.min(\"date\").alias(\"min_date\"),\n",
    "            F.max(\"date\").alias(\"max_date\"),\n",
    "            F.count(\"date\").alias(\"total_days\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\nüìÖ Date Range:\")\n",
    "        print(f\"   From: {date_stats['min_date']}\")\n",
    "        print(f\"   To: {date_stats['max_date']}\")\n",
    "        print(f\"   Total trading days: {date_stats['total_days']:,}\")\n",
    "        \n",
    "        # Feature value ranges\n",
    "        print(f\"\\nüìè Feature Ranges:\")\n",
    "        for col in ['daily_return', 'volatility_7', 'momentum']:\n",
    "            stats = df.select(\n",
    "                F.min(col).alias('min_val'),\n",
    "                F.max(col).alias('max_val'),\n",
    "                F.avg(col).alias('avg_val')\n",
    "            ).collect()[0]\n",
    "            \n",
    "            print(f\"   {col}: [{stats['min_val']:.4f}, {stats['max_val']:.4f}] (avg: {stats['avg_val']:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing {table_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6703f6f-186f-450b-a890-bdac271796ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Verification Queries\n",
    "\n",
    "Run some verification queries to ensure data consistency and feature correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51f41a4-6096-4405-aa47-6967408be2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run verification queries\n",
    "print(\"üîç Data Verification Queries\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if feature_tables:\n",
    "    # Query 1: Check if daily returns are calculated correctly\n",
    "    print(\"\\nüìä Query 1: Daily Return Calculation Verification\")\n",
    "    print(\"Checking if daily_return = (close - prev_close) / prev_close\")\n",
    "    \n",
    "    for table_name in feature_tables[:1]:  # Check first table only\n",
    "        ticker = table_name.split('_')[-1]\n",
    "        \n",
    "        verification_query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            date,\n",
    "            close,\n",
    "            LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date) as prev_close,\n",
    "            daily_return,\n",
    "            ROUND(\n",
    "                (close - LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date)) / \n",
    "                LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date), \n",
    "                6\n",
    "            ) as calculated_return\n",
    "        FROM {table_name}\n",
    "        WHERE date >= (SELECT MAX(date) - INTERVAL 7 DAYS FROM {table_name})\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = spark_session.sql(verification_query)\n",
    "            print(f\"\\n{ticker} - Recent daily returns:\")\n",
    "            result.show(truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in verification query: {str(e)}\")\n",
    "\n",
    "    # Query 2: Check moving averages\n",
    "    print(\"\\nüìà Query 2: Moving Average Verification\")\n",
    "    print(\"Checking 7-day and 30-day moving averages\")\n",
    "    \n",
    "    for table_name in feature_tables[:1]:  # Check first table only\n",
    "        ticker = table_name.split('_')[-1]\n",
    "        \n",
    "        ma_query = f\"\"\"\n",
    "        SELECT \n",
    "            date,\n",
    "            close,\n",
    "            moving_avg_7,\n",
    "            moving_avg_30,\n",
    "            ROUND(\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY ticker \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "                ), 2\n",
    "            ) as calculated_ma7\n",
    "        FROM {table_name}\n",
    "        WHERE date >= (SELECT MAX(date) - INTERVAL 10 DAYS FROM {table_name})\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = spark_session.sql(ma_query)\n",
    "            print(f\"\\n{ticker} - Recent moving averages:\")\n",
    "            result.show(truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in moving average query: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No feature tables available for verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d347b1-dc7e-412b-b9a1-57b14c3fbb06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Summarize the feature engineering results and provide guidance for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92711f65-b2d9-4588-b2a4-88d0c4c3f00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üìã Feature Engineering Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(f\"\\n‚úÖ Feature Engineering Completed Successfully\")\n",
    "    print(f\"   - Target tickers: {', '.join(target_tickers)}\")\n",
    "    print(f\"   - Successfully processed: {len(results['processed_tickers'])} tickers\")\n",
    "    print(f\"   - Failed: {len(results['failed_tickers'])} tickers\")\n",
    "    print(f\"   - Total features created: {results['total_features_created']:,}\")\n",
    "    print(f\"   - Processing time: {(results['end_time'] - results['start_time']).total_seconds():.2f} seconds\")\n",
    "    \n",
    "    if results['processed_tickers']:\n",
    "        print(f\"\\nüìä Available Feature Tables:\")\n",
    "        for ticker in results['processed_tickers']:\n",
    "            print(f\"   - portfolio_catalog.portfolio_schema.features_{ticker}\")\n",
    "            \n",
    "    print(f\"\\nüéØ Created Features:\")\n",
    "    feature_list = [\n",
    "        \"daily_return (Daily price return)\",\n",
    "        \"moving_avg_7 (7-day moving average)\",\n",
    "        \"moving_avg_30 (30-day moving average)\",\n",
    "        \"volatility_7 (7-day rolling volatility)\",\n",
    "        \"momentum (Price momentum indicator)\",\n",
    "        \"feature_timestamp (Feature creation date)\"\n",
    "    ]\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        print(f\"   ‚úÖ {feature}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Feature engineering did not complete successfully\")\n",
    "    print(\"   Please check the error messages above and retry\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Review the generated features for data quality\")\n",
    "print(f\"   2. Use these feature tables for ML model training\")\n",
    "print(f\"   3. Set up scheduled jobs for regular feature updates\")\n",
    "print(f\"   4. Consider adding more advanced features (technical indicators, etc.)\")\n",
    "print(f\"   5. Implement feature monitoring and alerting\")\n",
    "\n",
    "print(f\"\\n‚ú® Feature engineering notebook completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
