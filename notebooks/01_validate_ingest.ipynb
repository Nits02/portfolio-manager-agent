{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67b08a3",
   "metadata": {},
   "source": [
    "# Validate Financial Data Ingestion\n",
    "\n",
    "This notebook validates the data ingested into the bronze layer table `finance_catalog.bronze.prices`. \n",
    "We'll check:\n",
    "1. Data structure and sample content\n",
    "2. List of symbols ingested\n",
    "3. Date range coverage per symbol\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Set the catalog\n",
    "spark.sql(\"USE CATALOG finance_catalog\")\n",
    "print(\"Current catalog:\", spark.sql(\"SELECT current_catalog()\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475eada2",
   "metadata": {},
   "source": [
    "## Read Bronze Table Data\n",
    "\n",
    "Read the ingested data from the bronze layer prices table and create a temporary view for SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb08ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the prices table\n",
    "prices_df = spark.table(\"bronze.prices\")\n",
    "prices_df.createOrReplaceTempView(\"prices_view\")\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(\"Total number of records:\", prices_df.count())\n",
    "print(\"\\nSchema:\")\n",
    "prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde3bd2",
   "metadata": {},
   "source": [
    "## Display Sample Data\n",
    "\n",
    "Show a sample of 10 rows to inspect the data structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 sample rows ordered by date\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM prices_view\n",
    "    ORDER BY date DESC\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7f935",
   "metadata": {},
   "source": [
    "## Analyze Distinct Symbols\n",
    "\n",
    "Get a count of unique symbols and display the list of all tickers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct symbols and their record counts\n",
    "symbol_counts = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ticker,\n",
    "        COUNT(*) as record_count\n",
    "    FROM prices_view\n",
    "    GROUP BY ticker\n",
    "    ORDER BY record_count DESC\n",
    "\"\"\")\n",
    "\n",
    "display(symbol_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51a516",
   "metadata": {},
   "source": [
    "## Calculate Date Ranges\n",
    "\n",
    "Analyze the date range coverage for each symbol to ensure data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate date ranges per symbol\n",
    "date_ranges = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ticker,\n",
    "        MIN(date) as start_date,\n",
    "        MAX(date) as end_date,\n",
    "        COUNT(DISTINCT date) as trading_days,\n",
    "        DATEDIFF(MAX(date), MIN(date)) as total_days\n",
    "    FROM prices_view\n",
    "    GROUP BY ticker\n",
    "    ORDER BY ticker\n",
    "\"\"\")\n",
    "\n",
    "display(date_ranges)\n",
    "\n",
    "# Calculate any gaps in data\n",
    "gaps_analysis = spark.sql(\"\"\"\n",
    "    WITH dates AS (\n",
    "        SELECT \n",
    "            ticker,\n",
    "            date,\n",
    "            LAG(date) OVER (PARTITION BY ticker ORDER BY date) as prev_date\n",
    "        FROM prices_view\n",
    "    )\n",
    "    SELECT \n",
    "        ticker,\n",
    "        date as gap_end,\n",
    "        prev_date as gap_start,\n",
    "        DATEDIFF(date, prev_date) as days_gap\n",
    "    FROM dates\n",
    "    WHERE DATEDIFF(date, prev_date) > 1\n",
    "    ORDER BY ticker, date\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nChecking for gaps in data:\")\n",
    "display(gaps_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb94c90",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This validation notebook has:\n",
    "1. Confirmed data structure and content through sample display\n",
    "2. Identified all unique symbols in the dataset with their record counts\n",
    "3. Analyzed date ranges for each symbol\n",
    "4. Detected any gaps in the trading data\n",
    "\n",
    "Next steps:\n",
    "- Review any gaps in data to determine if they are holidays or missing data\n",
    "- Check for any symbols with unexpectedly low record counts\n",
    "- Verify that adjusted close prices are properly recorded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb87f0c",
   "metadata": {},
   "source": [
    "# Validate Financial Data Ingestion\n",
    "\n",
    "This notebook validates the data ingested into the bronze layer table `finance_catalog.bronze.prices`. \n",
    "We'll check:\n",
    "1. Data structure and sample content\n",
    "2. List of symbols ingested\n",
    "3. Date range coverage per symbol\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba426b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Set the catalog\n",
    "spark.sql(\"USE CATALOG finance_catalog\")\n",
    "print(\"Current catalog:\", spark.sql(\"SELECT current_catalog()\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75caa8a",
   "metadata": {},
   "source": [
    "## Read Bronze Table Data\n",
    "\n",
    "Read the ingested data from the bronze layer prices table and create a temporary view for SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the prices table\n",
    "prices_df = spark.table(\"bronze.prices\")\n",
    "prices_df.createOrReplaceTempView(\"prices_view\")\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(\"Total number of records:\", prices_df.count())\n",
    "print(\"\\nSchema:\")\n",
    "prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3197c",
   "metadata": {},
   "source": [
    "## Display Sample Data\n",
    "\n",
    "Show a sample of 10 rows to inspect the data structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 sample rows ordered by date\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM prices_view\n",
    "    ORDER BY date DESC\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ab68e",
   "metadata": {},
   "source": [
    "## Analyze Distinct Symbols\n",
    "\n",
    "Get a count of unique symbols and display the list of all tickers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct symbols and their record counts\n",
    "symbol_counts = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ticker,\n",
    "        COUNT(*) as record_count\n",
    "    FROM prices_view\n",
    "    GROUP BY ticker\n",
    "    ORDER BY record_count DESC\n",
    "\"\"\")\n",
    "\n",
    "display(symbol_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a062dd9",
   "metadata": {},
   "source": [
    "## Calculate Date Ranges\n",
    "\n",
    "Analyze the date range coverage for each symbol to ensure data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d87750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate date ranges per symbol\n",
    "date_ranges = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ticker,\n",
    "        MIN(date) as start_date,\n",
    "        MAX(date) as end_date,\n",
    "        COUNT(DISTINCT date) as trading_days,\n",
    "        DATEDIFF(MAX(date), MIN(date)) as total_days\n",
    "    FROM prices_view\n",
    "    GROUP BY ticker\n",
    "    ORDER BY ticker\n",
    "\"\"\")\n",
    "\n",
    "display(date_ranges)\n",
    "\n",
    "# Calculate any gaps in data\n",
    "gaps_analysis = spark.sql(\"\"\"\n",
    "    WITH dates AS (\n",
    "        SELECT \n",
    "            ticker,\n",
    "            date,\n",
    "            LAG(date) OVER (PARTITION BY ticker ORDER BY date) as prev_date\n",
    "        FROM prices_view\n",
    "    )\n",
    "    SELECT \n",
    "        ticker,\n",
    "        date as gap_end,\n",
    "        prev_date as gap_start,\n",
    "        DATEDIFF(date, prev_date) as days_gap\n",
    "    FROM dates\n",
    "    WHERE DATEDIFF(date, prev_date) > 1\n",
    "    ORDER BY ticker, date\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nChecking for gaps in data:\")\n",
    "display(gaps_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c156d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This validation notebook has:\n",
    "1. Confirmed data structure and content through sample display\n",
    "2. Identified all unique symbols in the dataset with their record counts\n",
    "3. Analyzed date ranges for each symbol\n",
    "4. Detected any gaps in the trading data\n",
    "\n",
    "Next steps:\n",
    "- Review any gaps in data to determine if they are holidays or missing data\n",
    "- Check for any symbols with unexpectedly low record counts\n",
    "- Verify that adjusted close prices are properly recorded"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
